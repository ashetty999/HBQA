{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# E2 = Experiment 2 on T5\n",
        "- Changed Question Length to 150 (from 250)\n",
        "- Changed Answer Length to 500 (from 32)\n",
        "- Changed Batch to 4 (from 2)\n",
        "- Changing Training Approach."
      ],
      "metadata": {
        "id": "xhMxInMCGaYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libararies"
      ],
      "metadata": {
        "id": "2jZzbP7b8iw1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2VacrsXBLqEc"
      },
      "outputs": [],
      "source": [
        "# !pip install -Uq transformers\n",
        "# !pip install -Uq evaluate\n",
        "# !pip install -Uq SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate>=0.20.1\n",
        "# !pip install transformers[torch]\n",
        "# # You need to restart the kernel after this step"
      ],
      "metadata": {
        "id": "WbDoP7u6NSeJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCruv7vhoP2w"
      },
      "source": [
        "# Load Configuration & Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LebXgjoFAzv",
        "outputId": "5de69e36-bc4a-4a69-b3c5-25d60d87c041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dGeIvSekE9L9"
      },
      "outputs": [],
      "source": [
        "apipath = r'H:\\\\My Drive\\\\config\\\\hbqa.txt'\n",
        "apipath = r'/content/drive/MyDrive/config/hbqa-colab.txt'\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(apipath)\n",
        "secret_key = config['global']['OPENAI_KEY']\n",
        "datapath = config['global']['DATA_FOLDER']\n",
        "corpuspath = config['global']['CORPUS_FOLDER']\n",
        "PINECONE_API_KEY = config['global']['PINECONE_KEY']\n",
        "PINECONE_ENV = config['global']['PINECONE_ENV']\n",
        "\n",
        "# PINECONE is Vector Database. To store the vector so that we can quickly search the vector space.\n",
        "# https://app.pinecone.io\n",
        "# get PINECONE_API_KEY key from app.pinecone.io\n",
        "# find your PINECONE_ENVIRONMENT next to the api key in pinecone console\n",
        "\n",
        "modelpath  = \"/content/drive/MyDrive/HBQA/T5QA_E2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GhdQxLkpiRC7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import transformers\n",
        "import evaluate  # Bleu\n",
        "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f3wqesBaSHQY"
      },
      "outputs": [],
      "source": [
        "# # Detect and initialize TPU\n",
        "# tpu_available = tf.config.experimental.list_logical_devices(\"TPU\")\n",
        "# if tpu_available:\n",
        "#     print(\"TPU available\")\n",
        "# else:\n",
        "#     print(\"No TPU available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yaVUMHxj_jkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d8dfb7-b747-469f-c512-779330d577a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qYZeJ16zoGDQ"
      },
      "outputs": [],
      "source": [
        "# sample code\n",
        "# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "# TOKENIZER = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n",
        "# MODEL = AutoModelForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n",
        "# MODEL.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVUo67UeySRL"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f1Uf713Y8QsS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fxdVz7jQFG2U"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(datapath+\"06-HBQA_Manual_with_Chunk.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "qdSgYnT519ED",
        "outputId": "8829d9a5-c467-49a8-9f37-6564556da5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1104, 9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ques_Id  Chunk_Id  Section_Id  \\\n",
              "0    10000     10193  Book01_046   \n",
              "1    10001     10193  Book01_046   \n",
              "2    10002     10193  Book01_046   \n",
              "3    10003     10193  Book01_046   \n",
              "\n",
              "                                            Question  \\\n",
              "0  Why did the Muni wander over the earth and wee...   \n",
              "1          Who did the Muni ask for a bride and why?   \n",
              "2  Who informed Vasuki about the Muni's desire fo...   \n",
              "3  How did Vasuki respond when he heard about the...   \n",
              "\n",
              "                                          Ref_Answer  \\\n",
              "0  The Muni wandered over the earth and wept loud...   \n",
              "1  The Muni asked for a bride from the creatures ...   \n",
              "2  The snakes that had been set upon Jaratkaru's ...   \n",
              "3  Vasuki took a maiden, who was decked with orna...   \n",
              "\n",
              "                                               Chunk  \\\n",
              "0  Sauti continued, 'The Muni, having said so unt...   \n",
              "1  Sauti continued, 'The Muni, having said so unt...   \n",
              "2  Sauti continued, 'The Muni, having said so unt...   \n",
              "3  Sauti continued, 'The Muni, having said so unt...   \n",
              "\n",
              "                                           Reference  WordsInQues  WordsInAns  \n",
              "0  \"'O ye, being directed by my ancestors, I am r...           14          25  \n",
              "1  \"My ancestors, afflicted with grief, have dire...           10          26  \n",
              "2  \"Then those snakes that had been set upon Jara...           10          20  \n",
              "3  \"And the king of the snakes, hearing their wor...           14          22  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-efcdbb7a-4fd0-49da-9fcf-f21284feae5f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ques_Id</th>\n",
              "      <th>Chunk_Id</th>\n",
              "      <th>Section_Id</th>\n",
              "      <th>Question</th>\n",
              "      <th>Ref_Answer</th>\n",
              "      <th>Chunk</th>\n",
              "      <th>Reference</th>\n",
              "      <th>WordsInQues</th>\n",
              "      <th>WordsInAns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>10193</td>\n",
              "      <td>Book01_046</td>\n",
              "      <td>Why did the Muni wander over the earth and wee...</td>\n",
              "      <td>The Muni wandered over the earth and wept loud...</td>\n",
              "      <td>Sauti continued, 'The Muni, having said so unt...</td>\n",
              "      <td>\"'O ye, being directed by my ancestors, I am r...</td>\n",
              "      <td>14</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>10193</td>\n",
              "      <td>Book01_046</td>\n",
              "      <td>Who did the Muni ask for a bride and why?</td>\n",
              "      <td>The Muni asked for a bride from the creatures ...</td>\n",
              "      <td>Sauti continued, 'The Muni, having said so unt...</td>\n",
              "      <td>\"My ancestors, afflicted with grief, have dire...</td>\n",
              "      <td>10</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>10193</td>\n",
              "      <td>Book01_046</td>\n",
              "      <td>Who informed Vasuki about the Muni's desire fo...</td>\n",
              "      <td>The snakes that had been set upon Jaratkaru's ...</td>\n",
              "      <td>Sauti continued, 'The Muni, having said so unt...</td>\n",
              "      <td>\"Then those snakes that had been set upon Jara...</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>10193</td>\n",
              "      <td>Book01_046</td>\n",
              "      <td>How did Vasuki respond when he heard about the...</td>\n",
              "      <td>Vasuki took a maiden, who was decked with orna...</td>\n",
              "      <td>Sauti continued, 'The Muni, having said so unt...</td>\n",
              "      <td>\"And the king of the snakes, hearing their wor...</td>\n",
              "      <td>14</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-efcdbb7a-4fd0-49da-9fcf-f21284feae5f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-efcdbb7a-4fd0-49da-9fcf-f21284feae5f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-efcdbb7a-4fd0-49da-9fcf-f21284feae5f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-42fe4a66-54d2-4a9e-8705-a04d946b37e1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42fe4a66-54d2-4a9e-8705-a04d946b37e1')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-42fe4a66-54d2-4a9e-8705-a04d946b37e1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "print(df.shape)\n",
        "df.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt2QQ5J7Ccvn",
        "outputId": "975ab7b7-0cbf-44ec-e7ba-2f22de8966b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 222)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Question_Len = int(max(len(ques) for ques in df.Question)/4)\n",
        "Answer_Len = int(max(len(ans) for ans in df.Ref_Answer)/4)\n",
        "Question_Len,Answer_Len # in Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uCXnNdcA9fsP"
      },
      "outputs": [],
      "source": [
        "Q_LEN =  150 #256   # Question Length\n",
        "T_LEN =  500 #32  # Target Length\n",
        "BATCH_SIZE = 4\n",
        "# DEVICE = \"cuda:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCYSDPpOoVI5"
      },
      "source": [
        "# Load Base Model for Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HJVp3iCbiug9"
      },
      "outputs": [],
      "source": [
        "class QA_Dataset(Dataset):\n",
        "  def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
        "      self.tokenizer = tokenizer\n",
        "      self.q_len = q_len\n",
        "      self.t_len = t_len\n",
        "      self.data = dataframe\n",
        "      self.questions = self.data[\"Question\"]\n",
        "      self.context = self.data[\"Chunk\"]\n",
        "      self.answer = self.data['Ref_Answer']\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.questions)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      question = self.questions[idx]\n",
        "      context = self.context[idx]\n",
        "      answer = self.answer[idx]\n",
        "\n",
        "      question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
        "                                                  truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "      answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n",
        "                                        truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "\n",
        "      labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
        "      labels[labels == 0] = -100\n",
        "\n",
        "      return {\n",
        "          \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
        "          \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
        "          \"labels\": labels,\n",
        "          \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Pyg3Qnok-gEl"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n",
        "model.to(DEVICE)\n",
        "optimizer = Adam(model.parameters(), lr=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Earlier Experiment code\n",
        "# # Dataloader\n",
        "\n",
        "# train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# train_sampler = RandomSampler(train_data.index)\n",
        "# val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "# qa_dataset = QA_Dataset(tokenizer, df, Q_LEN, T_LEN)\n",
        "\n",
        "# train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "# val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
      ],
      "metadata": {
        "id": "e0e_M3e2QNmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WY_lp1B19tpc"
      },
      "outputs": [],
      "source": [
        "#E2 Experiment Code\n",
        "# Dataloader\n",
        "\n",
        "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_sampler = RandomSampler(train_data.index)\n",
        "val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "train_dataset = QA_Dataset(tokenizer, df.loc[train_sampler,:], Q_LEN, T_LEN)\n",
        "val_dataset = QA_Dataset(tokenizer, df.loc[val_sampler,:], Q_LEN, T_LEN)\n",
        "\n",
        "# train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "# val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE0WSRmUL3pI"
      },
      "source": [
        "# Start finetune (Training)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaTN_D45JdY-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Approach"
      ],
      "metadata": {
        "id": "dYRgzr18JHKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "Wt5b68TbHESr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDLx6WAf-A3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "a239f5ab-07f7-40f9-d3ad-1a5dfccc5c52"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='5525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   3/5525 00:51 < 78:28:02, 0.02 it/s, Epoch 0.01/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='5525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  26/5525 17:27 < 66:40:11, 0.02 it/s, Epoch 0.11/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# # Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=modelpath,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=25,\n",
        "    save_steps=10000, # Set the frequency to save the model\n",
        "    eval_steps=10000,  # Set the frequency to evaluate the model\n",
        "    save_total_limit=1, # Limit the number of saved checkpoints to 1 (best model)\n",
        "    logging_dir='./logs', # learning_rate=2e-5,\n",
        "    evaluation_strategy=\"epoch\",  # Change this to \"steps\"/ \"epoch\" if you want to evaluate at the end of each epoch\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,  # Set to True if you want to push the model to the Hugging Face Hub\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another Method (used in experiment 1)"
      ],
      "metadata": {
        "id": "wE3D4tk9IsSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loss = 0\n",
        "# val_loss = 0\n",
        "# train_batch_count = 0\n",
        "# val_batch_count = 0\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     model.train()\n",
        "#     for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
        "#         input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "#         attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "#         labels = batch[\"labels\"].to(DEVICE)\n",
        "#         decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "#         outputs = model(\n",
        "#                           input_ids=input_ids,\n",
        "#                           attention_mask=attention_mask,\n",
        "#                           labels=labels,\n",
        "#                           decoder_attention_mask=decoder_attention_mask\n",
        "#                         )\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs.loss.backward()\n",
        "#         optimizer.step()\n",
        "#         train_loss += outputs.loss.item()\n",
        "#         train_batch_count += 1\n",
        "\n",
        "#     #Evaluation\n",
        "#     model.eval()\n",
        "#     for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
        "#         input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "#         attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "#         labels = batch[\"labels\"].to(DEVICE)\n",
        "#         decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "#         outputs = MODEL(\n",
        "#                           input_ids=input_ids,\n",
        "#                           attention_mask=attention_mask,\n",
        "#                           labels=labels,\n",
        "#                           decoder_attention_mask=decoder_attention_mask\n",
        "#                         )\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs.loss.backward()\n",
        "#         optimizer.step()\n",
        "#         val_loss += outputs.loss.item()\n",
        "#         val_batch_count += 1\n",
        "\n",
        "#     print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
      ],
      "metadata": {
        "id": "idfILk4kIrAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8WhoU_2hdnx"
      },
      "source": [
        "# Save/ Load Finetuned (trained) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgrrJ4Pkp6Fk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXSNsTcxpFz5"
      },
      "source": [
        "## Save Finetuned (trained) Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khdNtjawUV_X"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(modelpath)\n",
        "tokenizer.save_pretrained(modelpath)\n",
        "\n",
        "# Saved files\n",
        "\"\"\"('qa_tokenizer/tokenizer_config.json',\n",
        " 'qa_tokenizer/special_tokens_map.json',\n",
        " 'qa_tokenizer/spiece.model',\n",
        "'qa_tokenizer/added_tokens.json',\n",
        "'qa_tokenizer/tokenizer.json')\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBit8QnZg4iF"
      },
      "source": [
        "## Load Model from Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWTckpGQg38e"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the corresponding tokenizer\n",
        "TOKENIZER = T5Tokenizer.from_pretrained(modelpath)\n",
        "\n",
        "# Load the pre-trained T5 model\n",
        "MODEL = T5ForConditionalGeneration.from_pretrained(modelpath)\n",
        "MODEL.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suQoNy6t2ruM"
      },
      "outputs": [],
      "source": [
        "# input_text = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
        "\n",
        "# # Tokenize the input text\n",
        "# input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "# input_ids = input_ids.to(DEVICE)\n",
        "\n",
        "# # Generate translated text\n",
        "# translated_ids = MODEL.generate(input_ids)\n",
        "\n",
        "# # Decode the generated IDs back to text\n",
        "# translated_text = TOKENIZER.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"Translated Text:\", translated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "832Ayr0HC95L"
      },
      "source": [
        "# Predict Answers from t5 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Gq3uQTA0DS"
      },
      "outputs": [],
      "source": [
        "def predict_answer(context, question):\n",
        "\n",
        "    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
        "\n",
        "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
        "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
        "\n",
        "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
        "    return predicted_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VryuVFvSWGr4"
      },
      "source": [
        "## Check Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHyYhQi6g4GJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "qno=[]\n",
        "N=5\n",
        "\n",
        "for i in range(N):\n",
        "    qno.append(random.randint(0,df.shape[0] ) )\n",
        "\n",
        "pred_answers=[]\n",
        "ref_answers=[]\n",
        "question=[]\n",
        "\n",
        "for i in qno:\n",
        "    chunk    = df.iloc[i]['Chunk'],\n",
        "    ques     = df.iloc[i]['Question']\n",
        "    ref_ans  = df.iloc[i]['Ref_Answer']\n",
        "\n",
        "    pred_ans = predict_answer(chunk, ques)\n",
        "\n",
        "    pred_answers.append(pred_ans)\n",
        "    ref_answers.append(ref_ans)\n",
        "    question.append(ques)\n",
        "\n",
        "    print('Question  :', ques)\n",
        "    print(\"Ref Answer:\", ref_ans)\n",
        "    print(\"Pred Ans  :\", pred_ans)\n",
        "    print('--------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IZq8iCPWLTL"
      },
      "source": [
        "## Predict All Answer & Save Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcpzVER1XWdK"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "pred_answers=[]\n",
        "ref_answers=[]\n",
        "questions=[]\n",
        "for i in range(df.shape[0]):\n",
        "    chunk    = df.iloc[i]['Chunk'],\n",
        "    ques     = df.iloc[i]['Question']\n",
        "    ref_ans  = df.iloc[i]['Ref_Answer']\n",
        "\n",
        "    pred_ans = predict_answer(chunk, ques)\n",
        "\n",
        "    pred_answers.append(pred_ans)\n",
        "    ref_answers.append(ref_ans)\n",
        "    questions.append(ques)\n",
        "\n",
        "    # print('Question  :', ques)\n",
        "    # print(\"Ref Answer:\", ref_ans)\n",
        "    # print(\"Pred Ans  :\", pred_ans)\n",
        "    # print('--------')\n",
        "    print(f\"Predicting ans for question {df.iloc[i]['Ques_Id']}\")\n",
        "    df.loc[i,'T5Pred_Answer'] = pred_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WtNurcbEctC"
      },
      "outputs": [],
      "source": [
        "df[['Ques_Id','Ref_Answer','T5_Pred_Answer']].to_csv(datapath + '09.11-t5Predicted_ans.csv')\n",
        "# df= pd.read_csv(datapath + '11.1-t5Predicted_ans.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate and Save Metrics - 1"
      ],
      "metadata": {
        "id": "bpxEwvEA-Plg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tABXVsrWhW03"
      },
      "outputs": [],
      "source": [
        "def calculate_score(ref_answer, predicted_answer):\n",
        "\n",
        "  pred_answer_ids = TOKENIZER.encode(predicted_answer, return_tensors='pt')[0][0]\n",
        "  pred_answer_ids = pred_answer_ids.to(DEVICE)\n",
        "\n",
        "  ref_answer_ids = TOKENIZER.encode(ref_answer, return_tensors='pt')[0][0]\n",
        "  ref_answer_ids = pred_answer_ids.to(DEVICE)\n",
        "\n",
        "  bleu = evaluate.load(\"google_bleu\")\n",
        "  bleu_score1  = bleu.compute(predictions=[predicted_answer], references=[ref_answer])\n",
        "\n",
        "  # squad = evaluate.load(\"squad\")\n",
        "  glue_qqp = evaluate.load('glue', 'qqp')\n",
        "\n",
        "  glue_qqp_score1 = glue_qqp.compute(predictions=[pred_answer_ids],\n",
        "                      references=[ref_answer_ids])\n",
        "\n",
        "  return bleu_score1, glue_qqp_score1 #squad_score1 #bleu_score1#, squad_score1, glue_score1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHV47f_pwVIY"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(columns = ['Ques_Id','BLEU_Score','GLUE_Acc','GLUE_F1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhEK-0Acx_PA"
      },
      "outputs": [],
      "source": [
        "bleu_score=[]\n",
        "glue_qqp_score=[]\n",
        "from datasets import load_metric\n",
        "\n",
        "N= df.shape[0]\n",
        "\n",
        "for i in range(N):\n",
        "  # bleu_score1 = calculate_score( df1.iloc[i]['ref_answer'],df1.iloc[i]['pred_answer'])\n",
        "  ques_id = df.loc[i,'Ques_Id']\n",
        "  ref_ans  = df.loc[i,'Ref_Answer']\n",
        "  pred_ans = df.loc[i,'T5_Pred_Answer']\n",
        "\n",
        "  bleu, glue = calculate_score(ref_ans, pred_ans)\n",
        "  print(bleu,glue,ques_id)\n",
        "\n",
        "  bleu_score.append(bleu['google_bleu'])\n",
        "  glue_qqp_score.append(glue['accuracy'])\n",
        "  glue_qqp_score.append(glue['f1'])\n",
        "\n",
        "  df1.loc[i] = (ques_id, bleu['google_bleu'], glue['accuracy'], glue['f1'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmI9Ic0kxXwq"
      },
      "outputs": [],
      "source": [
        "df1.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CvsXd2qzUt_"
      },
      "outputs": [],
      "source": [
        "df1.to_csv(datapath + '09.12-t5Predicted_Ans_Score_E2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5pkA8XPFypt"
      },
      "outputs": [],
      "source": [
        "df1[[ 'BLEU_Score', 'GLUE_Acc', 'GLUE_F1']].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding/Vecotrizing Predicted Answer"
      ],
      "metadata": {
        "id": "tvwWuiwI7Es-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_t5Predicted_Ans = pd.read_csv(datapath+\"09.11-t5Predicted_ans_E2.csv\")\n",
        "df_t5Predicted_Ans"
      ],
      "metadata": {
        "id": "1-Loh4W7Q1gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "wc2Tq6kxZx3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "T5PredAns_Sentences = df_t5Predicted_Ans.T5_Pred_Answer.tolist()\n",
        "T5PredAns_Embeddings = model.encode(T5PredAns_Sentences)\n",
        "\n",
        "T5PredAnsVectors= torch.tensor(T5PredAns_Embeddings, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "T5PredAnsVec_list = T5PredAnsVectors.tolist()\n",
        "\n",
        "df_t5Predicted_Ans['T5_AnsVector'] = T5PredAnsVec_list\n",
        "\n",
        "df_t5Predicted_Ans.to_csv(datapath+'09.11-t5Predicted_AnsVector_E2')"
      ],
      "metadata": {
        "id": "MD3ytLZqXooJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_t5Predicted_Ans.shape)\n",
        "df_t5Predicted_Ans.head(3)"
      ],
      "metadata": {
        "id": "xhjYnjd_XoSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate and Save Metrics -2"
      ],
      "metadata": {
        "id": "thMJwH2q_nS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_t5Predicted_Ans = pd.read_csv(datapath+'09.11-t5Predicted_AnsVector', usecols=['Ques_Id','T5_AnsVector'])"
      ],
      "metadata": {
        "id": "j5d-noPu_xWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ref_ans = pd.read_csv(datapath + '07.2-HBQA_QA_Vector.csv', usecols=['Ques_Id','AnsVector'])"
      ],
      "metadata": {
        "id": "yA3D5-KbXoHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined=df_t5Predicted_Ans.merge(df_ref_ans, on=\"Ques_Id\", how=\"inner\")"
      ],
      "metadata": {
        "id": "rpmphzvA7sWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_combined.shape)\n",
        "df_combined.head(3)\n"
      ],
      "metadata": {
        "id": "Ysf8DZo-7sQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_cosine(row):\n",
        "  # Remove the brackets and split the string by commas\n",
        "  predAns_vector_values = row['T5_AnsVector'].strip('[]').split(',')\n",
        "  refAns_vector_values  = row['AnsVector'].strip('[]').split(',')\n",
        "\n",
        "\n",
        "  # Convert the string values to floats\n",
        "  predAns_vector_values = [float(value) for value in predAns_vector_values]\n",
        "  refAns_vector_values  = [float(value) for value in refAns_vector_values]\n",
        "\n",
        "\n",
        "  # Convert tensor\n",
        "  predAns_vector_values = torch.tensor(predAns_vector_values).reshape(1, -1)\n",
        "  refAns_vector_values = torch.tensor(refAns_vector_values).reshape(1, -1)\n",
        "\n",
        "  # print(predAns_vector_values)\n",
        "  # print('----')\n",
        "  # print (refAns_vector_values)\n",
        "  # Calculate Cosine\n",
        "  return round(F.cosine_similarity(predAns_vector_values,refAns_vector_values).item(),3)\n"
      ],
      "metadata": {
        "id": "uG3rEbzdCBec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rowid in df_combined.index:\n",
        "  df_combined.loc[rowid,'CosineSim'] = calculate_cosine( df_combined.loc[rowid])"
      ],
      "metadata": {
        "id": "HYi4vQ2i7sMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined.CosineSim.mean()"
      ],
      "metadata": {
        "id": "nDtuCW87IZJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6_8GuXfw1yX"
      },
      "outputs": [],
      "source": [
        "# sst2, mnli, mnli_mismatched, mnli_matched, qnli, rte, wnli, cola,stsb, mrpc, qqp, and hans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HguahnXowLWQ"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "# glue_metric = load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
        "# references = [0, 1]\n",
        "# predictions = [0, 1]\n",
        "# results = glue_metric.compute(predictions=predictions, references=references)\n",
        "# print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGBKBRuewmcl"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "# glue_metric = load('glue', 'stsb')\n",
        "# references = [0., 1., 2., 3., 4., 5.]\n",
        "# predictions = [-10., -11., -12., -13., -14., -15.]\n",
        "# results = glue_metric.compute(predictions=predictions, references=references)\n",
        "# print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AlyJn-mwp8G"
      },
      "outputs": [],
      "source": [
        "# from evaluate import load\n",
        "# glue_metric = load('glue', 'cola')\n",
        "# references = [0, 1]\n",
        "# predictions = [1, 1]\n",
        "# results = glue_metric.compute(predictions=predictions, references=references)\n",
        "# results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}