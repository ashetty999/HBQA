{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hari_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hari_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "directory = 'MBBook-Txt'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('QA_Final.csv')\n",
    "ques = list(df['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \"\\n\".join(ques).replace('Question: ','').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "c:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)e9125/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 1.17MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 63.6kB/s]\n",
      "Downloading (…)7e55de9125/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 9.18MB/s]\n",
      "Downloading (…)55de9125/config.json: 100%|██████████| 612/612 [00:00<00:00, 153kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 38.7kB/s]\n",
      "Downloading (…)125/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 198kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:23<00:00, 3.84MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 55.0kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading (…)e9125/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 725kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 362kB/s]\n",
      "Downloading (…)9125/train_script.py: 100%|██████████| 13.2k/13.2k [00:00<?, ?B/s]\n",
      "Downloading (…)7e55de9125/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 561kB/s]\n",
      "Downloading (…)5de9125/modules.json: 100%|██████████| 349/349 [00:00<00:00, 174kB/s]\n",
      "c:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_utils.py:147: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xe . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem . (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  t = torch.tensor([], dtype=storage.dtype, device=storage._untyped_storage.device)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvectorstores\u001b[39;00m \u001b[39mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 2\u001b[0m db \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_documents(documents, embeddings)\n",
      "File \u001b[1;32mc:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:489\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m texts \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    488\u001b[0m metadatas \u001b[39m=\u001b[39m [doc\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 489\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[0;32m    490\u001b[0m     texts\u001b[39m=\u001b[39;49mtexts,\n\u001b[0;32m    491\u001b[0m     embedding\u001b[39m=\u001b[39;49membedding,\n\u001b[0;32m    492\u001b[0m     metadatas\u001b[39m=\u001b[39;49mmetadatas,\n\u001b[0;32m    493\u001b[0m     ids\u001b[39m=\u001b[39;49mids,\n\u001b[0;32m    494\u001b[0m     collection_name\u001b[39m=\u001b[39;49mcollection_name,\n\u001b[0;32m    495\u001b[0m     persist_directory\u001b[39m=\u001b[39;49mpersist_directory,\n\u001b[0;32m    496\u001b[0m     client_settings\u001b[39m=\u001b[39;49mclient_settings,\n\u001b[0;32m    497\u001b[0m     client\u001b[39m=\u001b[39;49mclient,\n\u001b[0;32m    498\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:457\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[39mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    450\u001b[0m chroma_collection \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    451\u001b[0m     collection_name\u001b[39m=\u001b[39mcollection_name,\n\u001b[0;32m    452\u001b[0m     embedding_function\u001b[39m=\u001b[39membedding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m     client\u001b[39m=\u001b[39mclient,\n\u001b[0;32m    456\u001b[0m )\n\u001b[1;32m--> 457\u001b[0m chroma_collection\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mtexts, metadatas\u001b[39m=\u001b[39;49mmetadatas, ids\u001b[39m=\u001b[39;49mids)\n\u001b[0;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32mc:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:151\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(\u001b[39mlist\u001b[39;49m(texts))\n\u001b[0;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collection\u001b[39m.\u001b[39mupsert(\n\u001b[0;32m    153\u001b[0m     metadatas\u001b[39m=\u001b[39mmetadatas, embeddings\u001b[39m=\u001b[39membeddings, documents\u001b[39m=\u001b[39mtexts, ids\u001b[39m=\u001b[39mids\n\u001b[0;32m    154\u001b[0m )\n\u001b[0;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "File \u001b[1;32mc:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\langchain\\embeddings\\huggingface.py:78\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m), texts))\n\u001b[1;32m---> 78\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mencode(texts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_kwargs)\n\u001b[0;32m     79\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:197\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    195\u001b[0m     all_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(all_embeddings)\n\u001b[0;32m    196\u001b[0m \u001b[39melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m--> 197\u001b[0m     all_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([emb\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m all_embeddings])\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m input_was_string:\n\u001b[0;32m    200\u001b[0m     all_embeddings \u001b[39m=\u001b[39m all_embeddings[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\hari_\\anaconda3\\envs\\myenv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:197\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    195\u001b[0m     all_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(all_embeddings)\n\u001b[0;32m    196\u001b[0m \u001b[39melif\u001b[39;00m convert_to_numpy:\n\u001b[1;32m--> 197\u001b[0m     all_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([emb\u001b[39m.\u001b[39;49mnumpy() \u001b[39mfor\u001b[39;00m emb \u001b[39min\u001b[39;00m all_embeddings])\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m input_was_string:\n\u001b[0;32m    200\u001b[0m     all_embeddings \u001b[39m=\u001b[39m all_embeddings[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
